import google.generativeai as genai
from src.llm.data import Prompt
from src.llm.configs import LLM_CONFIGS
genai.configure(api_key=LLM_CONFIGS['GEMINI_API_KEY'])
# class LLM:
#     def __init__(self, api_key, model_name='gemini-1.5-flash-latest'):
#         """
#         Initializes the LLM with the necessary API configuration and model selection.

#         Parameters:
#             api_key (str): The API key required to authenticate requests to the generative AI service.
#             model_name (str): The name of the model to be used for generating responses.
#         """
        
#         self.model = genai.GenerativeModel(model_name)

#     def generate_response(self, query:str, prompt:Prompt, stream=False):
#         """
#         Generates a response from the LLM based on the provided prompt and document context.

#         Parameters:
#             prompt (str): The prompt or question to which the model should respond.
#             stream (bool): Indicates whether the response should be streamed (for large responses).

#         Returns:
#             str: The response generated by the LLM.
#         """
#         input = prompt.get_prompt(input=query)
#         if stream:
#             text_response = []
#             response_generator = self.model.generate_content(input, stream=True)
#             for chunk in response_generator:
#                 text_response.append(chunk.text)
#                 yield chunk.text
#             prompt.add_history(query, ''.join(text_response))
#         else:
#             response = self.model.generate_content(input, stream=stream)
#             prompt.add_history(query, response)
#             return response



class LLM:
    def __init__(self, model_name='gemini-1.5-flash-latest'):
        """
        Initializes the LLM with the necessary API configuration and model selection.

        Parameters:
            api_key (str): The API key required to authenticate requests to the generative AI service.
            model_name (str): The name of the model to be used for generating responses.
        """
        
        self.model = genai.GenerativeModel(model_name)

    def generate_response(self, query:str, prompt:Prompt, stream=False):
        """
        Generates a response from the LLM based on the provided prompt and document context.

        Parameters:
            prompt (str): The prompt or question to which the model should respond.
            stream (bool): Indicates whether the response should be streamed (for large responses).

        Returns:
            str: The response generated by the LLM.
        """
        input = prompt.get_prompt(input=query)
        if stream:
            text_response = []
            response_generator = self.model.generate_content(input, stream=True)
            for chunk in response_generator:
                text_response.append(chunk.text)
                yield chunk.text
            prompt.add_history(query, ''.join(text_response))
        else:
            response = self.model.generate_content(input, stream=stream)
            prompt.add_history(query, response)
            return response
    def generate_response_(self, query:str, stream=False):
        """
        Generates a response from the LLM based on the provided prompt and document context.

        Parameters:
            prompt (str): The prompt or question to which the model should respond.
            stream (bool): Indicates whether the response should be streamed (for large responses).

        Returns:
            str: The response generated by the LLM.
        """
        if stream:
            text_response = []
            response_generator = self.model.generate_content(input, stream=True)
            for chunk in response_generator:
                text_response.append(chunk.text)
                yield chunk.text
            # prompt.add_history(query, ''.join(text_response))
        else:
            response = self.model.generate_content(input, stream=stream)
            # prompt.add_history(query, response)
            return response
