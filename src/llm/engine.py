import google.generativeai as genai
from data import Prompt
genai.configure(api_key='AIzaSyAVDR6tcYRjOKOlwGw6qfPUX32ul30UmyI')
class LLM:
    def __init__(self, api_key, model_name='gemini-1.5-flash-latest'):
        """
        Initializes the LLM with the necessary API configuration and model selection.

        Parameters:
            api_key (str): The API key required to authenticate requests to the generative AI service.
            model_name (str): The name of the model to be used for generating responses.
        """
        
        self.model = genai.GenerativeModel(model_name)
        self.document = None
 
    # def load_document(self, file_path):
    #     """
    #     Loads a PDF document, merges its context for use as a reference in generating responses.

    #     Parameters:
    #         file_path (str): Path to the PDF file to load.
    #     """
    #     loader = SinglePDFLoader(file=file_path, clean=True)
    #     self.document = merge_context(loader)

    def generate_response(self, query:str, prompt:Prompt, stream=False):
        """
        Generates a response from the LLM based on the provided prompt and document context.

        Parameters:
            prompt (str): The prompt or question to which the model should respond.
            stream (bool): Indicates whether the response should be streamed (for large responses).

        Returns:
            str: The response generated by the LLM.
        """
        input = prompt.get_prompt(input=query)
        if stream:
            text_response = []
            response_generator = self.model.generate_content(input, stream=True)
            for chunk in response_generator:
                text_response.append(chunk.text)
                yield chunk
            prompt.add_history(query, ''.join(text_response))
        else:
            response = self.model.generate_content(input, stream=stream)
            prompt.add_history(query, response)
            return response
